head(data)
names(housing)
housing["FES "]
housing[,"FES "]
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile = paste0(getwd(), '/getdata%2Fdata%2FDATA.gov_NGAP.xlsx'), method = "curl")
dat <- xlsx::read.xlsx(file = "getdata%2Fdata%2FDATA.gov_NGAP.xlsx", sheetIndex = 1, rowIndex = 18:23, colIndex = 7:15)
################ Quiz1#############
library(xlsx)
?xlsx
install.packages("xlsx")
dat <- xlsx::read.xlsx(file = "getdata%2Fdata%2FDATA.gov_NGAP.xlsx", sheetIndex = 1, rowIndex = 18:23, colIndex = 7:15)
sum(dat$Zip*dat$Ext,na.rm=T)
install.packages("XML")
library("XML")
fileURL<-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- XML::xmlTreeParse(sub("s", "", fileURL), useInternal = TRUE)
rootNode <- XML::xmlRoot(doc)
zipcodes <- XML::xpathSApply(rootNode, "//zipcode", XML::xmlValue)
xmlZipcodeDT <- data.table::data.table(zipcode = zipcodes)
xmlZipcodeDT[zipcode == "21231", .N]
DT <- data.table::fread("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv")
# Answer (fastest):
system.time(DT[,mean(pwgtp15),by=SEX])
sapply(split(DT$pwgtp15,DT$SEX),mean)
DT[,mean(pwgtp15),by=SEX]
rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2]
tapply(DT$pwgtp15,DT$SEX,mean)
mean(DT$pwgtp15,by=DT$SEX)
mean(DT[DT$SEX==1,]$pwgtp15)
mean(DT[DT$SEX==2,]$pwgtp15)
rowMeans(DT)[DT$SEX==2]
DT <- fread("./Dataset/Quiz1-05.csv")
system.time(rowMeans(DT)[DT$SEX==2])
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(DT[,mean(pwgtp15),by=SEX])
dateDownloaded <- date()
dateDownloaded
library(httr)
library(httr)
###Find OAuth settings for github: http://developer.github.com/v3/oauth/
oauth_endpoints("github")
myapp <- oauth_app("github",
key = "75ffc4989df8001de43a",
secret = "389877827ca7031f4586a37206816ec5152088dc")
###Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
stop_for_status(req)
output <- content(req)
##Find “datasharing”
datashare <- which(sapply(output, FUN=function(X) "datasharing" %in% X))
datashare
list(output[[15]]$name, output[[15]]$created_at)
###Find OAuth settings for github: http://developer.github.com/v3/oauth/
oauth_endpoints("github")
myapp <- oauth_app(" Coursera_github",
key = "c762f186ed4fca05927d",
secret = "e10081779974d5793071482dad09cb25e5542010")
###Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app(" Alka_Yadav",
key = "c762f186ed4fca05927d",
secret = "e10081779974d5793071482dad09cb25e5542010")
###Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET("https://api.github.com/users/jtleek/repos", config(token = github_token))
stop_for_status(req)
output <- content(req)
req
stop_for_status
output
##Find “datasharing”
datashare <- which(sapply(output, FUN=function(X) "datasharing" %in% X))
datashare
list(output[[15]]$name, output[[15]]$created_at)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile = "acs.csv")
acs <- read.csv("acs.csv")
head(acs)
detach("package:RMySQL", unload=TRUE)
detach("package:RMySQL", unload=TRUE)
sqldf("select pwgtp1 from acs where AGEP < 50")
#### Loading package
library(sqldf)
install.packages("sqldf")
#### Loading package
library(sqldf)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile = "acs.csv")
acs <- read.csv("acs.csv")
head(acs)
detach("package:RMySQL", unload=TRUE)
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select pwgtp1 from acs where AGEP < 50")
sqldf("select pwgtp1 from acs where AGEP < 50")
####Option B:
sqldf("select * from acs where AGEP < 50 and pwgtp1")
##Option C:
sqldf("select pwgtp1 from acs")
##Option D:
sqldf("select pwgtp1 from acs where AGEP < 50")
Z <- unique(acs$AGEP)
htmlUrl <- url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode <- readLines(htmlUrl)
close(htmlUrl)
head(htmlCode)
htmlCode(100)
htmlCode
c(nchar(htmlCode[10]), nchar(htmlCode[20]), nchar(htmlCode[30]), nchar(htmlCode[100]))
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
SST <- read.fwf(fileUrl, skip=4, widths=c(12, 7, 4, 9, 4, 9, 4, 9, 4))
lines <- readLines(url, n = 10)
lines <- readLines(fileUrl, n = 10)
colNames <- c("filler", "week", "filler", "sstNino12", "filler", "sstaNino12",
"filler", "sstNino3", "filler", "sstaNino3", "filler", "sstNino34", "filler",
"sstaNino34", "filler", "sstNino4", "filler", "sstaNino4")
d <- read.fwf(fileUrl, w, header = FALSE, skip = 4, col.names = colNames)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
lines <- readLines(url, n = 10)
w <- c(1, 9, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3, 5, 4, 1, 3)
colNames <- c("filler", "week", "filler", "sstNino12", "filler", "sstaNino12",
"filler", "sstNino3", "filler", "sstaNino3", "filler", "sstNino34", "filler",
"sstaNino34", "filler", "sstNino4", "filler", "sstaNino4")
d <- read.fwf(url, w, header = FALSE, skip = 4, col.names = colNames)
d <- d[, grep("^[^filler]", names(d))]
sum(d[, 4])
lines
d
d[, 4]
d <- read.fwf(url, w, header = FALSE, skip = 4, col.names = colNames)
d
d <- d[, grep("^[^filler]", names(d))]
sum(d[, 4])
swirl()
mydf<-read.csv(path2csv)
mydf<-read.csv(path2csv,stringsAsFactors = FALSE)
dim(mydf)
head(mydf)
library(dplyr)
package_version("dplyr")
packageVersion("dplyr")
cran<-tbl-df(mydf)
cran<-tbl_df(mydf)
rm(mydf)
rm("mydf")
cran
?select
select(cran, ip_id, package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran, -time)
-5:20
-(5:20)
select(cran, -(X:size))
filter(cran, package == "swirl")
filter(cran, r_version == "3.1.1", country == "US")
?Comparison
filter(cran, r_version <= "3.0.2", country == "IN")
filter(cran, country == "US" | country == "IN")
filter(cran, size > 100500, r_os == "linux-gnu")
is.na(c(3, 5, NA, 10))
!is.na(c(3, 5, NA, 10))
filter(cran,!is.na(r_version)
)
cran2<-select(cran, size:ip_id)
arrange(cran,ip_id)
arrange(cran2, ip_id)
arrange(cran2, desc(ip_id) )
arrange(cran2, package, ip_id)
arrange(cran2, country,desc(r_version), ip_id)
cran3<-select(cran,ip_id,package,size)
cran3
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3,correct_size)
mutate(cran3, correct_size = size + 1000)
sizr_mb<- mutate(cran3, size_mb = size / 2^20)
size_mb<- mutate(cran3, size_mb = size / 2^20)
cran3
mutate(cran3, size_gb = size_mb / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3,correct_size = size.1000)
mutate(cran3, correct_size = size + 1000)
mutate(cran3, size_mb = size / 2^20)
mutate(cran3, size_mb = size / 2^20, size_gb = size_mb / 2^10)
mutate(cran3, correct_size = size + 1000)
summarize(cran, avg_bytes = mean(size))
library(dplyr)
cran<-tbl_df(mydf)
rm("mydf")
cran
?group_by
by_package<-group_by(cran,package)
by_package
summarize(by_package,mean(size))
submit()
pack_sum
quantile(pack_sum$count, probs = 0.99)
top_counts<-filter(pack_sum,count>679)
top_counts
View(top_counts)
top_counts_sorted<-arrange(top_counts,desc(count))
View(top_counts_sorted)
quantile(pack_sum$unique, prob=0.99)
top_unique<-filter(pack_sum,unique>465)
View(top_unique)
top_unique_sorted<-arrange(top_unique,desc(unique))
View(top_unique_sorted)
submit()
submit()
submit()
View(result3)
submit()
submit()
submit()
submit()
submit()
library(tidyr)
students
?gather
gather(students,sex,count,-grade)
students2
res<-gather(students2,sex_class,count,-grade)
res
?separate
separate(res, col = sex_class, into = c("sex", "class"))
submit()
submit()
students3
submit()
submit()
?spread
submit()
library(redr)
library(readr)
parse_number("class5")
submit()
submit()
students4
submit()
submit()
submit()
passed
failed
mutate(passed,status="passed")
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status = "failed")
bind_rows(passed,failed)
sat
submit()
submit()
submit()
submit()
submit()
submit()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = lubridate)
this_day<-today()
this_day
year(this_day)
wday(this_day)
wday(this_day,label = TRUE)
wday(this_day,label = TRUE)
wday(this_day,label=TRUE)
wday(this_day,label=TRUE)
wday(this_day)
wday(this_day, label = TRUE)
wday("this_day", label = TRUE)
wday(this_day)
wday(this_day,label = TRUE)
info()
wday(this_day, label = TRUE)
now()
wday(this_day,label = TRUE)
wday(this_day,label = TRUE)
wday(this_day,label = TRUE)
this_day
wday(this_day, label = TRUE)
wday(ymd("this_day"), label = TRUE)
library(tidyr)
library(lubridate)
wday(this_day, label = TRUE)
wday(this_day,label = TRUE)
wday(this_day,label = FALSE)
wday(this_day,label = TRUE)
info()
nxt()
wday(this_day,label=TRUE)
Q1Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
quit
exist
info()
skip()
swirl()
wday(this_day,label=TRUE)
info()
nxt()
bye()
Q1Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
Q1 <- read.csv(Q1Url)
head(Q1)
agricultureLogical <- Q1$ACR == 3 & Q1$AGS == 6
which(agricultureLogical)
#Use the parameter native=TRUE. What are the 30th and 80th quantiles of the resulting data?
#(some Linux systems may produce an answer 638 different for the 30th quantile)
library(jpeg)
Q2Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
Q2Path = '/home/cabunic/Data Science/Coursera/3 - Getting and Cleaning Data/Week 3/Q2.jpg'
download.file(Q2Url, Q2Path, mode = 'wb')
Q2 <- readJPEG(Q2Path, native = TRUE)
quantile(Q2, probs = c(0.3, 0.8))
paste(quantile(Q2, probs = 0.3) - 638, quantile(Q2, probs = 0.8))
getwd()
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/")
Q2Path = '/home/cabunic/Data Science/Coursera/3 - Getting and Cleaning Data/Week 3/Q2.jpg'
download.file(Q2Url, Q2Path, mode = 'wb')
Q2Path = 'C:/Users/Alka/Documents/R/datasciencecoursera.jpg'
download.file(Q2Url, Q2Path, mode = 'wb')
Q2 <- readJPEG(Q2Path, native = TRUE)
quantile(Q2, probs = c(0.3, 0.8))
library(dplyr)
library(data.table)
getwd()
Q3GDP_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q3GDP_Path <- "C:/Users/Alka/Documents/R/datasciencecoursera.csv"
Q3Edu_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
Q3Edu_Path <- "C:/Users/Alka/Documents/R/datasciencecoursera.csv"
download.file(Q3GDP_Url, Q3GDP_Path, method = "curl")
download.file(Q3Edu_Url, Q3Edu_Path, method = "curl")
Q3GDP <- fread(Q3GDP_Path, skip = 5, nrows = 190, select = c(1, 2, 4, 5), col.names = c("CountryCode", "Rank", "Economy", "Total"))
Q3Edu <- fread(Q3Edu_Path)
Q3GDP
Q3GDP_Path
Q3GDP_Url
Q3_Merge <- merge(Q3GDP, Q3Edu, by = 'CountryCode')
Q3_Merge <- Q3_Merge %>% arrange(desc(Rank))
Q3_Merge
Q3Edu
paste(nrow(Q3_Merge), " matches, 13th country is ", Q3_Merge$Economy[13])
##Question 4
##What is the average GDP ranking for the “High income: OECD” and “High income: nonOECD” group?
Q3_Merge %>% group_by(`Income Group`) %>%
filter("High income: OECD" %in% `Income Group` | "High income: nonOECD" %in% `Income Group`) %>%
summarize(Average = mean(Rank, na.rm = T)) %>%
arrange(desc(`Income Group`))
filter("High income: OECD" %in% `Income Group` | "High income: nonOECD" %in% `Income Group`)
Q3_Merge$RankGroups <- cut(Q3_Merge$Rank, breaks = 5)
swirl()
wday(this_day,label=TRUE)
bye()
swirl()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = lubridate)
this_day
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day,label = TRUE)
now()
wday(this_day, label = TRUE)
wday(this_day, label = TRUE)
wday(this_day, label = TRUE)
wday(this_day, label = TRUE)
bye()
Q1Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
Q1 <- read.csv(Q1Url)
Q1
Q1_colnames <- names(Q1)
Q1_colnames
strsplit(Q1_colnames, "^wgtp")[[123]]
strsplit(Q1_colnames, "^wgtp")
strsplit(Q1_colnames, "^wgtp")[[123]]
getwd()
Q2_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q2_Path <- "C:/Users/Alka/Documents/R/datasciencecoursera.csv"
download.file(Q2_Url, Q2_Path, method = "curl")
Q2_File <- read.csv(Q2_Path, nrow = 190, skip = 4)
Q2_File <- read.csv(Q2_Path, nrow = 190)
Q2_File
Q2_File <- read.csv(Q2_Path, nrow = 190, skip = 4)
Q2_File
Q2_File <- Q2_File[,c(1, 2, 4, 5)]
colnames(Q2_File) <- c("CountryCode", "Rank", "Country", "Total")
Q2_File
Q2_File$Total <- as.integer(gsub(",", "", Q2_File$Total))
Q2_File
mean(Q2_File$Total, na.rm = T)
Q2_File$Country
class(Q2_File$Country)
Q2_File$Country<-as.character(Q2_File$Country)
Q2_File$Country
Q2_File$Country[grep("^United", Q2_File$Country)]
getwd()
Q4GDP_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
Q4GDP_Path <- "C:/Users/Alka/Documents/R/datasciencecoursera.csv"
Q4Edu_Url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
Q4Edu_Path <- "C:/Users/Alka/Documents/R/datasciencecoursera.csv"
download.file(Q4GDP_Url, Q4GDP_Path, method = "curl")
download.file(Q4Edu_Url, Q4Edu_Path, method = "curl")
Q4GDP <- fread(Q4GDP_Path, skip = 5, nrows = 190, select = c(1, 2, 4, 5), col.names = c("CountryCode", "Rank", "Economy", "Total"))
Q4Edu <- fread(Q4Edu_Path)
Q4_Merge <- merge(Q4GDP, Q4Edu, by = 'CountryCode')
Q4_Merge
FiscalJune <- grep("Fiscal year end: June", Q4_Merge$`Special Notes`)
NROW(FiscalJune)
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
sampleTimes
amzn2012 <- sampleTimes[grep("^2012", sampleTimes)]
NROW(amzn2012)
NROW(amzn2012[weekdays(amzn2012) == "Monday"])
unzip(zipfile="./data/Dataset.zip",exdir="./data")
unzip(zipfile="./data/Dataset.zip",exdir="./data")
unzip(zipfile="getdata_projectfiles_UCI HAR Dataset.zip")
getwd()
list.files("C:/Users/Alka/Documents/R/datasciencecoursera")
getwd
unzip(zipfile="getdata_projectfiles_UCI HAR Dataset.zip")
setwd("C:\Users\Alka\Documents\R\datasciencecoursera\Peer-graded Assignment Getting and Cleaning Data Course Projec")
setwd("C:\Users\Alka\Documents\R\datasciencecoursera\Peer-graded Assignment Getting and Cleaning Data Course Projec\")
getwd()
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/Peer-graded Assignment Getting and Cleaning Data Course Projec/")
unzip(zipfile="getdata_projectfiles_UCI HAR Dataset.zip")
list.files("C:/Users/Alka/Documents/R/datasciencecoursera/Peer-graded Assignment Getting and Cleaning Data Course Projec/")
#define the path where the new folder has been unziped
pathdata = file.path("./midtermdata", "UCI HAR Dataset")
#create a file which has the 28 file names
files = list.files(pathdata, recursive=TRUE)
#show the files
files
#define the path where the new folder has been unziped
pathdata = file.path("C:/Users/Alka/Documents/R/datasciencecoursera/Peer-graded Assignment Getting and Cleaning Data Course Projec/", "UCI HAR Dataset")
#create a file which has the 28 file names
files = list.files(pathdata, recursive=TRUE)
#show the files
files
xtrain = read.table(file.path(pathdata, "train", "X_train.txt"),header = FALSE)
ytrain = read.table(file.path(pathdata, "train", "y_train.txt"),header = FALSE)
subject_train = read.table(file.path(pathdata, "train", "subject_train.txt"),header = FALSE)
#Reading the testing tables
xtest = read.table(file.path(pathdata, "test", "X_test.txt"),header = FALSE)
ytest = read.table(file.path(pathdata, "test", "y_test.txt"),header = FALSE)
subject_test = read.table(file.path(pathdata, "test", "subject_test.txt"),header = FALSE)
#Read the features data
features = read.table(file.path(pathdata, "features.txt"),header = FALSE)
#Read activity labels data
activityLabels = read.table(file.path(pathdata, "activity_labels.txt"),header = FALSE)
features
#Create Sanity and Column Values to the Train Data
colnames(xtrain) = features[,2]
head(xtrain)
head(ytrain)
colnames(ytrain) = "activityId"
head(ytrain)
head(xtrain)
colnames(subject_train) = "subjectId"
#Create Sanity and column values to the test data
colnames(xtest) = features[,2]
colnames(ytest) = "activityId"
colnames(subject_test) = "subjectId"
#Create sanity check for the activity labels value
colnames(activityLabels) <- c('activityId','activityType')
activityLabels
##### merge testdata and train data
mrg_train = cbind(ytrain, subject_train, xtrain)
mrg_test = cbind(ytest, subject_test, xtest)
dim(ytest)
dim(xtest)
dim(ytrain)
dim(subject_train)
dim(xtrain)
##### merge testdata and train data
mrg_train = cbind(ytrain, subject_train, xtrain)
mrg_test = cbind(ytest, subject_test, xtest)
#Create the main data table merging both table tables - this is the outcome of 1
setAllInOne = rbind(mrg_train, mrg_test)
head(setAllInOne)
head(setAllInOne)
# Need step is to read all the values that are available
colNames = colnames(setAllInOne)
colNames
#Need to get a subset of all the mean and standards and the correspondongin activityID and subjectID
mean_and_std = (grepl("activityId" , colNames) | grepl("subjectId" , colNames) | grepl("mean.." , colNames) | grepl("std.." , colNames))
mean_and_std
#A subtset has to be created to get the required dataset
setForMeanAndStd <- setAllInOne[ , mean_and_std == TRUE]
setForMeanAndStd
colnames(xtrain)
setWithActivityNames = merge(setForMeanAndStd, activityLabels, by='activityId', all.x=TRUE)
setWithActivityNames
# New tidy set has to be created
secTidySet <- aggregate(. ~subjectId + activityId, setWithActivityNames, mean)
secTidySet <- secTidySet[order(secTidySet$subjectId, secTidySet$activityId),]
#The last step is to write the ouput to a text file
write.table(secTidySet, "secTidySet.txt", row.name=FALSE)
getwd()
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/ProgrammingAssignment2")
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/Getting and Cleaning Data Course Projec/Project/")
getwd()
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/Getting and Cleaning data/Project/")
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/Getting and Cleaning data/Project/")
unzip(zipfile="getdata_projectfiles_UCI HAR Dataset.zip")
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/project/")
getwd()
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/Getting and cleaning data/")
setwd("C:/Users/Alka/Documents/R/datasciencecoursera/Getting and cleaning data/")
